# ?????
server.port=8080

# Ollama??
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=deepseek-r1:32b
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.chat.options.top-p=0.95
spring.ai.ollama.chat.options.top-k=40

# ????
logging.level.org.springframework.ai=DEBUG
logging.level.com.example.ollamademo=DEBUG

